{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7L5QaNNiXN7RBqAyM6hNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfzalKamboh/Web_Data_ETL_Pipeline/blob/main/Web_Data_ETL_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIf13j9xYJX-",
        "outputId": "d8aea7d8-2f2a-4407-8156-3bdccdf3d8bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkTWtUPdYjrl",
        "outputId": "586e44d4-8d4c-47ab-f6cb-99b71c8625f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "metadata": {
        "id": "vtDnLWhuYn47"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76jfJczeZBaI",
        "outputId": "e8469ef6-bcb1-4b6b-f8be-70bd66b67af0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process of Extracting text from any article on the web is start from here:**"
      ],
      "metadata": {
        "id": "6MkY_IsCaHpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WebScraper:\n",
        "  def __init__(self,url):\n",
        "    self.url = url\n",
        "\n",
        "  def extract_article_text(self):\n",
        "    response = requests.get(self.url)\n",
        "    html_content = response.content\n",
        "    su = BeautifulSoup(html_content,\"html.parser\")\n",
        "    article_text = su.get_text()\n",
        "    return article_text\n"
      ],
      "metadata": {
        "id": "d5PCAEgZZG4n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean and preprocess the text extracted from the article. Because i'm storing the frequency of each word in article**"
      ],
      "metadata": {
        "id": "ECVIpOaKbA61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "  def __init__(self,nltk_stopwords):\n",
        "    self.nltk_stopwords = nltk_stopwords\n",
        "\n",
        "  def tokenize_and_clean(self,text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
        "    return filtered_words"
      ],
      "metadata": {
        "id": "4_Q3alldaCY2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This class is for ETL (Extract, Transform, Load)**"
      ],
      "metadata": {
        "id": "0Ja-P3v0ca_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ETLPipeline:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def run(self):\n",
        "        scraper = WebScraper(self.url)\n",
        "        article_text = scraper.extract_article_text()\n",
        "\n",
        "        processor = TextProcessor(self.nltk_stopwords)\n",
        "        filtered_words = processor.tokenize_and_clean(article_text)\n",
        "\n",
        "        word_freq = Counter(filtered_words)\n",
        "        df = pd.DataFrame(word_freq.items(), columns=[\"Words\", \"Frequencies\"])\n",
        "        df = df.sort_values(by=\"Frequencies\", ascending=False)\n",
        "        return df"
      ],
      "metadata": {
        "id": "AZYbIbwHcsDG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run this pipeline to scrape textual data from any article from the web**"
      ],
      "metadata": {
        "id": "a1JXv5rIc1-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    article_url = \"https://en.wikipedia.org/wiki/Pakistan\"\n",
        "    pipeline = ETLPipeline(article_url)\n",
        "    result_df = pipeline.run()\n",
        "    print(result_df.head(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alh-TalwczYI",
        "outputId": "42d644a2-e16b-4764-d035-f482ef781f0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Words  Frequencies\n",
            "0          pakistan          532\n",
            "3221      retrieved          446\n",
            "3217           isbn          173\n",
            "3218       archived          173\n",
            "3219       original          172\n",
            "207        december          126\n",
            "57            world          124\n",
            "368         january          113\n",
            "56           muslim          112\n",
            "199           march          101\n",
            "876       pakistani           94\n",
            "2348       february           93\n",
            "1067          april           90\n",
            "140             may           85\n",
            "162        national           76\n",
            "201          august           75\n",
            "152         islamic           72\n",
            "136           south           70\n",
            "1309        october           66\n",
            "772       september           66\n",
            "369           first           66\n",
            "284         british           62\n",
            "1118  international           62\n",
            "252           india           61\n",
            "3220           july           60\n",
            "232      population           59\n",
            "3226              b           59\n",
            "159           state           59\n",
            "135         country           59\n",
            "732            june           58\n",
            "197          united           58\n",
            "86             also           58\n",
            "480      university           55\n",
            "4              main           54\n",
            "582             war           53\n",
            "53       government           51\n",
            "942         nuclear           51\n",
            "310             new           49\n",
            "463             one           48\n",
            "64         military           48\n",
            "234         largest           47\n",
            "41          history           47\n",
            "294          indian           46\n",
            "1022      countries           46\n",
            "1035       november           44\n",
            "341        economic           43\n",
            "233         million           43\n",
            "289         muslims           42\n",
            "762         kashmir           41\n",
            "947         foreign           41\n"
          ]
        }
      ]
    }
  ]
}